{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "    \n",
    "import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Input,Dense,Flatten,Embedding,Permute,Dot,Reshape\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "import math\n",
    " \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "    \n",
    "import scipy\n",
    "import statsmodels.api\n",
    "import statsmodels as sm\n",
    " \n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import re\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "import spacy\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 전처리 함수 및 Tokenizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_train_df(data):\n",
    "\n",
    "    encoding_result = DataFrame()\n",
    "\n",
    "    for i in data[\"essay_id\"].unique():\n",
    "        text = data.loc[data[\"essay_id\"] == i, \"essay\"]\n",
    "\n",
    "        pos_tagging = list(map(lambda x : \n",
    "                           [x.lemma_], \n",
    "                           sp(text.values[0])))\n",
    "\n",
    "        result_text = DataFrame(columns = [\"lemma\"])\n",
    "\n",
    "        for j in range(0, len(pos_tagging)):\n",
    "            result_text = pd.concat([result_text,\n",
    "                                          DataFrame(pos_tagging[j], \n",
    "                                                    columns = [\"lemma\"])])\n",
    "\n",
    "        morphs = list(result_text[\"lemma\"])\n",
    "\n",
    "        morphsVectored = list()\n",
    "\n",
    "        ## 다시 형태소 분석기를 돌려서, text의 문장을 형태소로 분해하여 morphs에 담는다.\n",
    "        ## 시간이 오래 걸립니다. 5분정도\n",
    "\n",
    "        for j in morphs:\n",
    "            temporailyList = list()\n",
    "\n",
    "            try:\n",
    "                # 색인사전에 있는 단어면 인덱스를 반환\n",
    "                temporailyList.append(vocabulary[j])\n",
    "            except KeyError:\n",
    "                # 없는 단어면 0으로 대체한다.\n",
    "                temporailyList.append(0)\n",
    "            morphsVectored.append(temporailyList)\n",
    "\n",
    "        vector_shell_text = np.array([0 for i in range(0, np.max(list(vocabulary.values())) + 1)])\n",
    "\n",
    "        vector_shell_text[np.reshape(np.array(morphsVectored),[-1])] = 1\n",
    "\n",
    "        encoding_result = pd.concat([encoding_result, DataFrame(vector_shell_text).T])\n",
    "\n",
    "\n",
    "    train_df = pd.concat([data, encoding_result.reset_index(drop = True)], axis = 1)\n",
    "\n",
    "    train_df.columns = train_df.columns.astype(\"str\")\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_ratio(data, score):\n",
    "\n",
    "    pos_tagging = [list(map(lambda x : \n",
    "                       [data.loc[i, \"essay_id\"], x.text, x.tag_, x.lemma_], \n",
    "                       sp(data[\"essay\"][i]))) \n",
    "              for i in range(0, len(data))]\n",
    "\n",
    "    result = DataFrame(columns = [\"essay_id\",\"word\",\"tag\",\"lemma\"])\n",
    "\n",
    "    for i in range(0, len(pos_tagging)):\n",
    "        result = pd.concat([result, DataFrame(pos_tagging[i], columns = [\"essay_id\",\"word\",\"tag\",\"lemma\"])])\n",
    "    \n",
    "    sen_count = data[\"essay\"].apply(lambda x : len(nltk.sent_tokenize(x))).reset_index(name = \"sen_count\")\n",
    "\n",
    "    sen_count = pd.concat([data[\"essay_id\"], data[\"essay\"].apply(lambda x : len(nltk.sent_tokenize(x)))], axis = 1)\n",
    "\n",
    "    sen_count.columns = [\"essay_id\",\"sen_count\"]\n",
    "\n",
    "    len_count = result.groupby(\"essay_id\")[\"lemma\"].count().reset_index()\n",
    "\n",
    "    len_count.columns = [\"essay_id\",\"len_count\"]\n",
    "\n",
    "    ## 각 품사를 총 단어 갯수로 나눔\n",
    "\n",
    "    count_result = DataFrame({\"essay_id\" : result[\"essay_id\"].unique()})\n",
    "    \n",
    "    for i in result[\"tag\"].unique():\n",
    "\n",
    "        test = result[result[\"tag\"] == i]\n",
    "\n",
    "        lemma_count = test.groupby(\"essay_id\")[\"lemma\"].count().reset_index()\n",
    "\n",
    "        count = pd.merge(lemma_count, len_count)\n",
    "\n",
    "        count[\"{}_ratio\".format(i)] = count[\"lemma\"] / count[\"len_count\"]\n",
    "\n",
    "        count_result = pd.merge(count_result, count, how = \"left\")\n",
    "\n",
    "    count_result = count_result.fillna(0)\n",
    "\n",
    "    count_result = pd.merge(count_result, data[[\"essay_id\",score]])\n",
    "    \n",
    "    return count_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_cos_simil(data, criterion):\n",
    "\n",
    "\n",
    "    pos_tagging = list(map(lambda x : \n",
    "                           [x.lemma_], \n",
    "                           sp(criterion)))\n",
    "\n",
    "    result_criterion = DataFrame(columns = [\"lemma\"])\n",
    "\n",
    "    for i in range(0, len(pos_tagging)):\n",
    "        result_criterion = pd.concat([result_criterion, \n",
    "                                      DataFrame(pos_tagging[i], \n",
    "                                                columns = [\"lemma\"])])\n",
    "\n",
    "    morphs = list(result_criterion[\"lemma\"])\n",
    "\n",
    "    morphsVectored = list()\n",
    "\n",
    "    ## 다시 형태소 분석기를 돌려서, text의 문장을 형태소로 분해하여 morphs에 담는다.\n",
    "\n",
    "    for i in morphs:\n",
    "        temporailyList = list()\n",
    "        #print(k)\n",
    "        try:\n",
    "            # 색인사전에 있는 단어면 인덱스를 반환\n",
    "            temporailyList.append(vocabulary[i])\n",
    "        except KeyError:\n",
    "            # 없는 단어면 0으로 대체한다.\n",
    "            temporailyList.append(0)\n",
    "        morphsVectored.append(temporailyList)\n",
    "\n",
    "\n",
    "    vector_shell = np.array([0 for i in range(0, np.max(list(vocabulary.values())) + 1)])\n",
    "\n",
    "    vector_shell[np.reshape(np.array(morphsVectored),[-1])] = 1\n",
    "\n",
    "    cos_simil = list()\n",
    "\n",
    "    for i in data[\"essay_id\"].unique():\n",
    "        text = data.loc[data[\"essay_id\"] == i, \"essay\"]\n",
    "\n",
    "        pos_tagging = list(map(lambda x : \n",
    "                           [x.lemma_], \n",
    "                           sp(text.values[0])))\n",
    "\n",
    "        result_text = DataFrame(columns = [\"lemma\"])\n",
    "\n",
    "        for j in range(0, len(pos_tagging)):\n",
    "            result_text = pd.concat([result_text,\n",
    "                                          DataFrame(pos_tagging[j], \n",
    "                                                    columns = [\"lemma\"])])\n",
    "\n",
    "        morphs = list(result_text[\"lemma\"])\n",
    "\n",
    "        morphsVectored = list()\n",
    "\n",
    "        ## 다시 형태소 분석기를 돌려서, text의 문장을 형태소로 분해하여 morphs에 담는다.\n",
    "        ## 시간이 오래 걸립니다. 5분정도\n",
    "\n",
    "        for j in morphs:\n",
    "            temporailyList = list()\n",
    "            #print(k)\n",
    "            try:\n",
    "                # 색인사전에 있는 단어면 인덱스를 반환\n",
    "                temporailyList.append(vocabulary[j])\n",
    "            except KeyError:\n",
    "                # 없는 단어면 0으로 대체한다.\n",
    "                temporailyList.append(0)\n",
    "            morphsVectored.append(temporailyList)\n",
    "\n",
    "        vector_shell_text = np.array([0 for i in range(0, np.max(list(vocabulary.values())) + 1)])\n",
    "\n",
    "        vector_shell_text[np.reshape(np.array(morphsVectored),[-1])] = 1\n",
    "\n",
    "        simil_result = vector_shell.dot(vector_shell_text) / (np.linalg.norm(vector_shell) * np.linalg.norm(vector_shell_text))\n",
    "\n",
    "        cos_simil.append(simil_result)\n",
    "\n",
    "        \n",
    "    return cos_simil   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Load 및 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = pd.read_csv(\"data/essay_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @정규표현식\n",
    "\n",
    "searcher = re.compile('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay[\"essay\"] = essay[\"essay\"].apply(lambda x : searcher.sub('',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_tokenizer(text):\n",
    "    return nltk.tokenize.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = essay[\"essay\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "## CountVectorizer로 문자 : 숫자로 이루어진 색인 사전을 만든다.\n",
    "## 이미 만들어진 색인사전을 이용해도 됩니다.\n",
    "\n",
    "vect = CountVectorizer(tokenizer = vect_tokenizer ,min_df = 1, analyzer = \"word\")\n",
    "vect.fit(text)\n",
    "vocabulary = vect.vocabulary_\n",
    "# CLS와 EOS 토큰을 각각 정의한다.\n",
    "vocabulary['CLS'] = len(vocabulary) + 1\n",
    "vocabulary['EOS'] = len(vocabulary) + 2\n",
    "DataFrame([vect.vocabulary_]).to_csv(\"data/색인사전.csv\",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리 및 학습, 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Essay_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-4bc8cbc26f67>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_1[\"sen_len\"] = essay_1[\"essay\"].apply(lambda x : len(x))\n"
     ]
    }
   ],
   "source": [
    "essay_1 = essay[essay[\"essay_set\"] == 1]\n",
    "\n",
    "essay_1[\"sen_len\"] = essay_1[\"essay\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = making_train_df(essay_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5403587443946188"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen_len 포함\n",
    "\n",
    "columns = [i for i in range(0, np.max(list(vocabulary.values())) + 1)]\n",
    "\n",
    "columns.extend([\"sen_len\"])\n",
    "\n",
    "columns = np.array(columns)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df_1[columns.astype(\"str\")], \n",
    "                                                  train_df_1[\"score\"],\n",
    "                                                 random_state = 42)\n",
    "\n",
    "rf_1 = RandomForestRegressor(n_jobs=-1, random_state = 42)\n",
    "\n",
    "rf_1.fit(train_x, train_y)\n",
    "\n",
    "predicted = np.round(rf_1.predict(val_x))\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(val_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Essay_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = making_train_df(essay_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_2 = essay[essay[\"essay_set\"] == 2].reset_index(drop = True)\n",
    "\n",
    "essay_2[\"sen_len\"] = essay_2[\"essay\"].apply(lambda x : len(x))\n",
    "\n",
    "essay_2_ratio = making_ratio(essay_2, \"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagging = [list(map(lambda x : \n",
    "                       [essay_2.loc[i, \"essay_id\"], x.text, x.pos_, x.tag_, x.lemma_, x.dep_], \n",
    "                       sp(essay_2[\"essay\"][i]))) \n",
    "              for i in range(0, len(essay_2))]\n",
    "\n",
    "result = DataFrame(columns = [\"essay_id\",\"word\",\"pos\",\"tag\",\"lemma\",\"depend\"])\n",
    "\n",
    "for i in range(0, len(pos_tagging)):\n",
    "    result = pd.concat([result, DataFrame(pos_tagging[i], columns = [\"essay_id\",\"word\",\"pos\",\"tag\",\"lemma\",\"depend\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_count = essay_2[\"essay\"].apply(lambda x : len(nltk.sent_tokenize(x))).reset_index(name = \"sen_count\")\n",
    "\n",
    "sen_count = pd.concat([essay_2[\"essay_id\"], essay_2[\"essay\"].apply(lambda x : len(nltk.sent_tokenize(x)))], axis = 1)\n",
    "\n",
    "sen_count.columns = [\"essay_id\",\"sen_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_count = result.groupby(\"essay_id\")[\"lemma\"].count().reset_index()\n",
    "\n",
    "len_count.columns = [\"essay_id\",\"len_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reset_index(drop = True)\n",
    "\n",
    "lemma_reason = DataFrame({\"pos\" : [\"SCONJ\"]})\n",
    "\n",
    "test = pd.merge(lemma_reason, result, how = \"left\")\n",
    "\n",
    "lemma_count = test.groupby(\"essay_id\")[\"lemma\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = pd.merge(lemma_count, sen_count)\n",
    "\n",
    "count[\"SCONJ_ratio\"] = count[\"lemma\"] / count[\"sen_count\"]\n",
    "\n",
    "count = pd.merge(count, essay_2[[\"essay_id\",\"score\"]])\n",
    "\n",
    "count = count[[\"essay_id\",\"lemma\",\"sen_count\",\"SCONJ_ratio\",\"score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reset_index(drop = True)\n",
    "\n",
    "lemma_reason = DataFrame({\"lemma\" : [\"if\",\"since\",\"when\",\"as\",\"because\",\"unless\"]})\n",
    "\n",
    "test = pd.merge(lemma_reason, result, how = \"left\")\n",
    "\n",
    "lemma_count = test.groupby(\"essay_id\")[\"lemma\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2 = pd.merge(lemma_count, sen_count)\n",
    "\n",
    "count_2[\"lemma_sen_ratio\"] = count_2[\"lemma\"] / count_2[\"sen_count\"]\n",
    "\n",
    "count_2 = pd.merge(count_2, essay_2[[\"essay_id\",\"score\"]])\n",
    "\n",
    "count_2 = count_2[[\"essay_id\",\"lemma\",\"sen_count\",\"lemma_sen_ratio\",\"score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = pd.merge(train_df_2,essay_2_ratio[[\"essay_id\",\"JJ_ratio\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = pd.merge(train_df_2, count[[\"essay_id\",\"SCONJ_ratio\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = pd.merge(train_df_2, count_2[[\"essay_id\",\"lemma_sen_ratio\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30180180180180183"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen_len, JJ_ratio포함\n",
    "\n",
    "columns = [i for i in range(0, 39755)]\n",
    "\n",
    "columns.extend([\"sen_len\",\"JJ_ratio\"])\n",
    "\n",
    "columns = np.array(columns)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df_2[columns.astype(\"str\")], \n",
    "                                                  train_df_2[\"score\"],\n",
    "                                                 random_state = 42)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1, random_state = 42)\n",
    "\n",
    "rf.fit(train_x, train_y)\n",
    "\n",
    "predicted = np.round(rf.predict(val_x))\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(val_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Essay_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_3 = essay[essay[\"essay_set\"] == 3].reset_index(drop = True)\n",
    "\n",
    "essay_3[\"sen_len\"] = essay_3[\"essay\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'The response should describe a mood of gratitude, love, similar appreciative mood. The response may include, but is not limited to: the author says he is “eternally grateful” to his parents for instilling in him a love of cooking. He also credits them for his appreciation of Cuban music \"which I adore to this day.\" In general he notes their having made an inviting home filled with \"endless celebrations\" out of \"modest\" means. the author credits his parents for instilling in him a great sense of \"family\" due to the \"environment\" they created. This sense of family extended to everyone in a time when the larger world was uninviting.the author mentions his family’s generosity in allowing others to stay with them and notes its reciprocal nature the author recognizes that his parents came to America \"selflessly\" in order to \"give their children a better life.\" He details their challenges and obstacles and observes that they \"endured.\" \"I will always be grateful to my parents for their love and sacrifice. I’ve often told them that what they did was a much morecourageous thing than I could have ever done.\" He mentions his admiration andhaving thanked them yet admits that he has, \"no way to express my gratitude.\" \"I will never forget that house or its gracious neighborhood or the many things I learned there about how to love. I will never forget how my parents turned this simple house into a home\"'\n",
    "\n",
    "cos_simil = making_cos_simil(essay_3, criterion)\n",
    "\n",
    "essay_3[\"cos_simil\"] = cos_simil\n",
    "\n",
    "train_df_3 = making_train_df(essay_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32666666666666666"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen_len, JJ_ratio포함\n",
    "\n",
    "columns = [i for i in range(0, 39755)]\n",
    "\n",
    "columns.extend([\"sen_len\"])\n",
    "\n",
    "columns = np.array(columns)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df_2[columns.astype(\"str\")], \n",
    "                                                  train_df_2[\"score\"],\n",
    "                                                 random_state = 42)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1, random_state = 42)\n",
    "\n",
    "rf.fit(train_x, train_y)\n",
    "\n",
    "predicted = np.round(rf.predict(val_x))\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(val_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Essay_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_4 = essay[essay[\"essay_set\"] == 4].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_4[\"sen_len\"] = essay_4[\"essay\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'The obstacles to dirigible docking include Building a mast on top of the building Meeting with engineers and dirigible engineers Transmitting the stress of the dirigible all the way down the building; the frame had to be shored up to the tune of $60,000 Housing the winches and other docking equipment Dealing with flammable gases Handling the violent air currents at the top of the building Confronting laws banning airships from the area Getting close enough to the building without puncturing'\n",
    "\n",
    "cos_simil = making_cos_simil(essay_4, criterion)\n",
    "\n",
    "essay_4[\"cos_simil\"] = cos_simil\n",
    "\n",
    "train_df_4 = making_train_df(essay_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33555555555555555"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cos_simil, sen_len 포함\n",
    "\n",
    "columns = [i for i in range(0, 39755)]\n",
    "\n",
    "columns.extend([\"cos_simil\", \"sen_len\"])\n",
    "\n",
    "columns = np.array(columns)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df_4[columns.astype(\"str\")], \n",
    "                                                  train_df_4[\"score\"],\n",
    "                                                 random_state = 42)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1, random_state = 42)\n",
    "\n",
    "rf.fit(train_x, train_y)\n",
    "\n",
    "predicted = np.round(rf.predict(val_x))\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(val_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Essay_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_5 = essay[essay[\"essay_set\"] == 5].reset_index(drop = True)\n",
    "\n",
    "essay_5[\"sen_len\"] = essay_5[\"essay\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'Being Patient Means that you are understanding and tolerant. A patient Person experiences difficulties without complaining.'\n",
    "\n",
    "cos_simil = making_cos_simil(essay_5, criterion)\n",
    "\n",
    "essay_5[\"cos_simil\"] = cos_simil\n",
    "\n",
    "train_df_5 = making_train_df(essay_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_5_ratio = making_ratio(essay_5, \"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_5 = pd.merge(train_df_5, essay_5_ratio[[\"essay_id\",\"NN_ratio\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3256997455470736"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen_len, nn_ratio 포함(Best Score)\n",
    "\n",
    "columns = [i for i in range(0, 39755)]\n",
    "\n",
    "columns.extend([\"sen_len\", \"NN_ratio\"])\n",
    "\n",
    "columns = np.array(columns)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df_5[columns.astype(\"str\")], \n",
    "                                                  train_df_5[\"score\"],\n",
    "                                                 random_state = 42)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1, random_state = 42)\n",
    "\n",
    "rf.fit(train_x, train_y)\n",
    "\n",
    "predicted = np.round(rf.predict(val_x))\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(val_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Essay_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_6 = essay[essay[\"essay_set\"] == 6].reset_index(drop = True)\n",
    "\n",
    "essay_6[\"sen_len\"] = essay_6[\"essay\"].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = '\"If you want a place in the sun, you will have to expect some blisters.\" Tell a true story about a time when this quote was true for you or someone you know.'\n",
    "\n",
    "cos_simil = making_cos_simil(essay_6, criterion)\n",
    "\n",
    "essay_6[\"cos_simil\"] = cos_simil\n",
    "\n",
    "train_df_6 = making_train_df(essay_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2154696132596685"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen_len, cos_simil,  포함\n",
    "\n",
    "columns = [i for i in range(0, 39755)]\n",
    "\n",
    "columns.extend([\"sen_len\", \"cos_simil\"])\n",
    "\n",
    "columns = np.array(columns)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_df_6[columns.astype(\"str\")], \n",
    "                                                  train_df_6[\"score\"],\n",
    "                                                 random_state = 42)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1, random_state = 42)\n",
    "\n",
    "rf.fit(train_x, train_y)\n",
    "\n",
    "predicted = np.round(rf.predict(val_x))\n",
    "\n",
    "sklearn.metrics.mean_absolute_error(val_y, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
